{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE boot camp notes\n",
    "** Mike Shvartsman **\n",
    "\n",
    "This guide assumes you can read through basic Python code or use your google skills to catch up on that as needed. We begin by understanding how tensorflow works. The key point to remember is that all the tensorflow computation happens in a graph, and all that you get to do in python is to manipulate and run that graph. This creates a programming paradigm that looks a lot like python but is actually quite different. We begin with a simple logistic regression example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.special import expit as logistic\n",
    "\n",
    "n_obs = 100\n",
    "n_features = 20\n",
    "\n",
    "x_ph = tf.placeholder(shape=(n_obs, n_features), name=\"x_ph\", dtype=tf.float32)\n",
    "beta_init = np.random.normal(size=(n_features, 1))\n",
    "beta_hat = tf.Variable(beta_init, dtype=tf.float32, name=\"beta\")\n",
    "p = tf.nn.sigmoid(tf.matmul(x_ph, beta_hat), name=\"yhat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize thie graph, and then explain what the different parts are: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TensorFlow Graph visualizer code\n",
    "# https://stackoverflow.com/questions/41388673/visualizing-a-tensorflow-graph-in-jupyter-doesnt-work\n",
    "import numpy as np\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = bytes(\"<stripped %d bytes>\"%size, 'utf-8')\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script src=\"//cdnjs.cloudflare.com/ajax/libs/polymer/0.3.3/platform.js\"></script>\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:800px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:800px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:800px;border:0\" srcdoc=\"\n",
       "        <script src=&quot;//cdnjs.cloudflare.com/ajax/libs/polymer/0.3.3/platform.js&quot;></script>\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.760423990584072&quot;).pbtxt = 'node {\\n  name: &quot;x_ph&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 100\\n        }\\n        dim {\\n          size: 20\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;beta/initial_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 20\\n          }\\n          dim {\\n            size: 1\\n          }\\n        }\\n        tensor_content: &quot;<stripped 80 bytes>&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;beta&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 20\\n        }\\n        dim {\\n          size: 1\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;beta/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;beta&quot;\\n  input: &quot;beta/initial_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@beta&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;beta/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;beta&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@beta&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;x_ph&quot;\\n  input: &quot;beta/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;yhat&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;MatMul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;x_ph_1&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 100\\n        }\\n        dim {\\n          size: 20\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;y_ph&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 100\\n        }\\n        dim {\\n          size: 1\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;beta_1/initial_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 20\\n          }\\n          dim {\\n            size: 1\\n          }\\n        }\\n        tensor_content: &quot;<stripped 80 bytes>&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;beta_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 20\\n        }\\n        dim {\\n          size: 1\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;beta_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;beta_1&quot;\\n  input: &quot;beta_1/initial_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@beta_1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;beta_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;beta_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@beta_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;x_ph_1&quot;\\n  input: &quot;beta_1/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;d\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Rank_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;d\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Sub/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;Rank_1&quot;\\n  input: &quot;Sub/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Slice/begin&quot;\\n  op: &quot;Pack&quot;\\n  input: &quot;Sub&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 1\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;axis&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Slice/size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;Shape_1&quot;\\n  input: &quot;Slice/begin&quot;\\n  input: &quot;Slice/size&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;concat/values_0&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: -1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;concat/values_0&quot;\\n  input: &quot;Slice&quot;\\n  input: &quot;concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;MatMul_1&quot;\\n  input: &quot;concat&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Rank_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Shape_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;d\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Sub_1/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Sub_1&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;Rank_2&quot;\\n  input: &quot;Sub_1/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Slice_1/begin&quot;\\n  op: &quot;Pack&quot;\\n  input: &quot;Sub_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 1\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;axis&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Slice_1/size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;Shape_2&quot;\\n  input: &quot;Slice_1/begin&quot;\\n  input: &quot;Slice_1/size&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;concat_1/values_0&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: -1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;concat_1/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;concat_1&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;concat_1/values_0&quot;\\n  input: &quot;Slice_1&quot;\\n  input: &quot;concat_1/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;y_ph&quot;\\n  input: &quot;concat_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;yhat_1&quot;\\n  op: &quot;SoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;Reshape&quot;\\n  input: &quot;Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Sub_2/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Sub_2&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;Rank&quot;\\n  input: &quot;Sub_2/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Slice_2/begin&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Slice_2/size&quot;\\n  op: &quot;Pack&quot;\\n  input: &quot;Sub_2&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 1\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;axis&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Slice_2&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;Shape&quot;\\n  input: &quot;Slice_2/begin&quot;\\n  input: &quot;Slice_2/size&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Reshape_2&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;yhat_1&quot;\\n  input: &quot;Slice_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;x_ph_2&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 100\\n        }\\n        dim {\\n          size: 20\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;beta_2/initial_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 20\\n          }\\n          dim {\\n            size: 1\\n          }\\n        }\\n        tensor_content: &quot;<stripped 80 bytes>&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;beta_2&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 20\\n        }\\n        dim {\\n          size: 1\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;beta_2/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;beta_2&quot;\\n  input: &quot;beta_2/initial_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@beta_2&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;beta_2/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;beta_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@beta_2&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;MatMul_2&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;x_ph_2&quot;\\n  input: &quot;beta_2/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;yhat_2&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;MatMul_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:800px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.760423990584072&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we do in tensorflow is construct graphs like this, and then evaluate nodes. Each graph node is associated with some code. When we evaluate a node like `y_hat`, tensorflow figures out what nodes it depends on, evaluates all of those nodes, and then evaluates `y_hat`. In this graph, there are three types of nodes (`Tensor`s), a `Variable`, a `placeholder`, and a vanilla `Tensor` that is none of the above. Tensors have no state: they are computable from the rest of the graph. Variables have state (they're the only thing we can optimize / save / load). Placeholders are not computable and don't have state: we must feed values into them. We can also feed values into other tensors, but TF will explicitly complain if we fail to feed a value into a placeholder. \n",
    "\n",
    "Our python objects like `beta_hat` are references to the TF graph nodes, not the nodes themselves (i.e. copying the python object does not dupliate the graph node). \n",
    "\n",
    "To evaluate a graph we need to associate it with a session, and then either a tensor's `eval` method or the session's `run` method. The difference is that we can `run` multiple tensors together, which might be useful if they share dependencies. Next we show how we can use our graph to generate synthetic data from our model. Note that to do so we feed a value for our `x` placeholder, but also feed a value for the `beta` tensor: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 0 0\n",
      "  1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 0 1 1\n",
      "  0 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 0 1 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.normal(size=(n_obs, n_features))\n",
    "beta = np.random.normal(size = (n_features, 1))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    p_simulated = p.eval(session=sess, feed_dict={beta_hat:beta, x_ph:x})\n",
    "    y_simulated = np.random.binomial(n=1, p=p_simulated)\n",
    "    \n",
    "print(y_simulated.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our loss function and optimize: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ph = tf.placeholder(shape=(n_obs, 1), name=\"y_ph\", dtype=tf.float32)\n",
    "\n",
    "# all the aggregator functions are reduce_* (reduce_sum, reduce_mean, reduce_max etc)\n",
    "logistic_loss = tf.reduce_sum(tf.log(y_ph * p) +tf.log(1-y_ph * 1-p))\n",
    "\n",
    "# if we needed the gradients for some reason, e.g. to pass to an external optimizier or to plot\n",
    "# grads = tf.gradients(logistic_loss, beta_hat)\n",
    "\n",
    "# \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\DeclareMathOperator{\\Tr}{Tr}\n",
    "\\newcommand{\\trp}{{^\\top}} % transpose\n",
    "\\newcommand{\\trace}{\\text{Trace}} % trace\n",
    "\\newcommand{\\inv}{^{-1}}\n",
    "\\newcommand{\\mb}{\\mathbf{b}}\n",
    "\\newcommand{\\M}{\\mathbf{M}}\n",
    "\\newcommand{\\G}{\\mathbf{G}}\n",
    "\\newcommand{\\A}{\\mathbf{A}}\n",
    "\\newcommand{\\R}{\\mathbf{R}}\n",
    "\\renewcommand{\\S}{\\mathbf{S}}\n",
    "\\newcommand{\\B}{\\mathbf{B}}\n",
    "\\newcommand{\\Q}{\\mathbf{Q}}\n",
    "\\newcommand{\\mH}{\\mathbf{H}}\n",
    "\\newcommand{\\U}{\\mathbf{U}}\n",
    "\\newcommand{\\mL}{\\mathbf{L}}\n",
    "\\newcommand{\\diag}{\\mathrm{diag}}\n",
    "\\newcommand{\\etr}{\\mathrm{etr}}\n",
    "\\renewcommand{\\H}{\\mathbf{H}}\n",
    "\\newcommand{\\vecop}{\\mathrm{vec}}\n",
    "\\newcommand{\\I}{\\mathbf{I}}\n",
    "\\newcommand{\\X}{\\mathbf{X}_{ij}}\n",
    "\\newcommand{\\Y}{\\mathbf{Y}_{jk}}\n",
    "\\newcommand{\\Z}{\\mathbf{Z}_{ik}}\n",
    "$$\n",
    "\n",
    "In our generative model, we would like to estimate the density of some complicated probability density $\\log P_{\\theta}(X)$. This is slightly odd notation but seems standard in these papers: it says that $X$ is a random variable but $\\theta$ are parameters. To do this, we will write it as follows:\n",
    "\n",
    "$$\n",
    "\\log P_{\\theta}(X) =  \\log P_{\\theta}(X,Z) - \\log P_{\\theta}(Z\\mid X) \n",
    "$$\n",
    "\n",
    "This is just the definition of marginal probability. Next, we add/subtract $\\log Q_{\\phi}(Z\\mid X)$ which sums to 0: \n",
    "\n",
    "$$\n",
    "\\log P_{\\theta}(X) =  \\log P_{\\theta}(X,Z) - \\log P_{\\theta}(Z\\mid X) - \\log Q_{\\phi}(Z\\mid X) + \\log Q_{\\phi}(Z\\mid X)\n",
    "$$\n",
    "\n",
    "We take expectation of both sides. Note that this expectation has to be w.r.t. the conditional distribution $Q(Z\\mid X)$ for the rest of this to work properly: \n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{Q(Z\\mid X)}\\log P_{\\theta}(X) =  \\mathbb{E}_{Q(Z\\mid X)}\\log P_{\\theta}(X,Z) - \\mathbb{E}_{Q(Z\\mid X)}\\log P_{\\theta}(Z\\mid X) - \\mathbb{E}_{Q(Z\\mid X)}\\log Q_{\\phi}(Z\\mid X) + \\mathbb{E}_{Q(Z\\mid X)}\\log Q_{\\phi}(Z\\mid X)\n",
    "$$\n",
    "\n",
    "Since the LHS is independent of Z, that expectation just goes away. We rearrange the terms and realize we ended up with the evidence lower bound (ELBO) and a KL divergence: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log P_{\\theta}(X) =&  \\mathbb{E}_{Q(Z\\mid X)}\\log P_{\\theta}(X,Z) - \\mathbb{E}_{Q(Z\\mid X)}\\log Q_{\\phi}(Z\\mid X) + \\mathbb{E}_{Q(Z\\mid X)}\\log Q_{\\phi}(Z\\mid X)- \\mathbb{E}_{Q(Z\\mid X)}\\log P_{\\theta}(Z\\mid X) \\\\\n",
    "  =&  \\mathbb{E}_{Q(Z\\mid X)}\\log P_{\\theta}(X,Z) - \\mathbb{E}_{Q(Z\\mid X)}\\log Q_{\\phi}(Z\\mid X) + \\int_Z Q_{\\phi}(Z\\mid X)\\log Q_{\\phi}(Z\\mid X)- \\int_Z Q_{\\phi}(Z\\mid X)\\log P_{\\theta}(Z\\mid X) \\\\\n",
    "=&  \\mathbb{E}_{Q(Z\\mid X)}\\log P_{\\theta}(X,Z) +\\mathbb{E}_{Q(Z\\mid X)}\\log Q_{\\phi}(Z\\mid X) - \\int_Z Q_{\\phi}(Z\\mid X)\\log \\frac{Q_{\\phi}(Z\\mid X)}{P_{\\theta}(Z\\mid X)} \\\\\n",
    "=&  \\mathbb{E}_{Q(Z\\mid X)}\\log P_{\\theta}(X,Z) + \\mathbb{E}_{Q(Z\\mid X)}\\log Q_{\\phi}(Z\\mid X) -\\mathcal{D}_{KL}(Q_{\\theta}(Z\\mid X)||  P_{\\phi}(Z\\mid X)) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The ELBO is a lower bound because KL divergence is greater than equal to 0. So if we want to maximize the LHS, we can choose to either minimize the KL divergence, or maximize the ELBO.  We would rather do the former, because the KL divergence contains the posterior $P(Z\\mid X)$ and if we knew how to do that, we wouldn't be going through this hassle. The nice thing is, since this holds for any $Q$ and any $Z$ we can define both distributions to be as nice as we like. So we're going to say that the likelihood $P_{\\theta}(X,Z)$, prior $P_{\\theta}(Z)$ and approximate posterior $Q_{\\theta}(X,Z)$ are all gaussian. We pick the easiest possible marginal distribution over $Q(Z)$, an identity-covariance gaussian: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P_{\\theta}(X\\mid Z) :=& \\mathcal{N}(a(Z,\\theta), b(Z,\\theta)b(Z,\\theta)\\trp) \\\\\n",
    "Q_{\\phi}(Z\\mid X):=&\\mathcal{N}(f(X,\\phi),g(X,\\phi)g(X,\\phi)^{\\intercal})\\\\\n",
    "P_{\\theta}(Z) :=& \\mathcal{N}(0,\\mathbf{I})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The distributions $P_{\\theta}(X\\mid Z)$ and $Q_{\\phi}(Z\\mid X)$ are parameterized by mean and covariance-square-root functions $a,b,f,g$ which we leave unspecified for now. In practice for VAEs people use the equivalent of a mean-field assumption, which means that the covariance functions will just return SDs/variances, but I'd like to write the reparameterization trick in general form. We can additionally rewrite the expression to get a second KL divergence: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log P_{\\theta}(X) =& \\mathbb{E}_{Q(Z\\mid X)}\\log P_{\\theta}(X\\mid Z) + \\mathbb{E}_{Q(Z\\mid X)}\\log P_{\\theta}(Z) + \\mathbb{E}_{Q(Z\\mid X)}\\log Q_{\\phi}(Z\\mid X)-\\mathcal{D}_{KL}(Q_{\\theta}(Z\\mid X)||  P_{\\phi}(Z\\mid X)) \\\\\n",
    "=&\\mathbb{E}_{Q(Z\\mid X)}\\log P_{\\theta}(X\\mid Z) + \\mathcal{D}_{KL}( Q_{\\phi}(Z\\mid X)||P_{\\theta}(Z)) - \\mathcal{D}_{KL}(Q_{\\theta}(Z\\mid X)||  P_{\\phi}(Z\\mid X))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Conveniently, the KL divergence between two gaussians (the prior and approximate posterior) is analytic. What remains is the expectation $\\mathbb{E}_{Q(Z\\mid X)}\\log P_{\\theta}(X\\mid Z)$, which we can compute from its empirical, sample-based mean: \n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{Q(Z\\mid X)}\\log P_{\\theta}(X\\mid Z) = \\int Q_{\\phi}(Z\\mid X)\\log P_{\\theta}(X|Z)d Q \\approx \\frac{1}{N} \\sum \\log P_{\\theta}(X|Z) Z_i, \\\\\\\n",
    "Z_i\\sim\\mathcal{N}(f(X,\\phi),g(X,\\phi)g(X,\\phi)\\trp)\n",
    "$$\n",
    "\n",
    "The naive gradient estimator here has very high variance according to the VAE paper, though it is used in Blei, Jordan and Paisley 2012 (ICML):\n",
    "\n",
    "$$\n",
    "\\nabla_{\\phi}\\mathbb{E}_{Q(Z\\mid X)}\\log P_{\\theta}(X\\mid Z) \\approx \\frac{1}{N} \\sum \\nabla_{\\phi}\\log P_{\\theta}(X|Z) Z_i\n",
    "$$\n",
    "\n",
    "What the VAE paper does instead is apply the reparameterization trick:  \n",
    "\n",
    "\\begin{align}\n",
    "\\epsilon&\\sim\\mathcal{N}(0, \\I)\\\\\n",
    "Z &= f(X,\\phi) + g(X,\\phi)\\epsilon\\\\\n",
    "\\mathbb{E}_{Q(Z\\mid X)}\\log P_{\\theta}(X\\mid Z)  &\\approx \\frac{1}{N} \\sum \\log P_{\\theta}(X|Z)(f(X,\\phi) + g(X,\\phi)\\epsilon)\\\\\n",
    "\\nabla_{\\phi}\\mathbb{E}_{Q(Z\\mid X)}\\log P_{\\theta}(X\\mid Z)  &\\approx \\frac{1}{N} \\sum \\nabla_{\\phi}\\log P_{\\theta}(X|Z)(f(X,\\phi) + g(X,\\phi)\\epsilon)\n",
    "\\end{align}\n",
    "\n",
    "Now let's see if we can implement it using tensorflow. We begin by sanity-checking a basic MLP and then go to VAEs. First, import some things we'll need and download MNIST: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "global_dtype = tf.float32\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "input_size = mnist.train.images.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define our neural network building blocks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _dense_mlp_layer(x, input_size, out_size, nonlinearity=tf.nn.softmax, name_prefix=\"\"):\n",
    "    w_init = tf.truncated_normal(shape=[input_size, out_size], stddev=0.001)\n",
    "    b_init = tf.ones(shape=[out_size]) * 0.1\n",
    "    W = tf.Variable(w_init, name=\"%s_W\" % name_prefix)\n",
    "    b = tf.Variable(b_init, name=\"%s_b\" % name_prefix)\n",
    "    out = nonlinearity(tf.matmul(x, W) + b)\n",
    "    return out, [W, b]\n",
    "\n",
    "def _mlp(x, n_layers, units_per_layer, input_size, out_size, nonlinearity=tf.tanh):\n",
    "    train_vars = []\n",
    "\n",
    "    x, v = _dense_mlp_layer(x, input_size, units_per_layer, nonlinearity, name_prefix=\"into_hidden\")\n",
    "    train_vars.extend(v)\n",
    "\n",
    "    for l in range(n_layers-1):\n",
    "        x, v = _dense_mlp_layer(x, units_per_layer, units_per_layer, nonlinearity, name_prefix=\"hidden\")\n",
    "        train_vars.extend(v)\n",
    "\n",
    "    x, v = _dense_mlp_layer(x, units_per_layer, out_size, nonlinearity, name_prefix=\"readout\")\n",
    "    train_vars.extend(v)\n",
    "    return x, train_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we construct the graph. The graph and scope boilerplate makes our life easier as far as visualization and debugging is concerned: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp_graph = tf.Graph()\n",
    "\n",
    "with mlp_graph.as_default():\n",
    "    with tf.name_scope(\"Feedforward_Net\"):\n",
    "        x = tf.placeholder(shape=[None, input_size], dtype=global_dtype, name='x')\n",
    "        y = tf.placeholder(shape=[None, 10], dtype=global_dtype, name='y')\n",
    "        y_hat, mlp_test_vars = _mlp(x, n_layers=2, units_per_layer=30, input_size=784, out_size=10)\n",
    "\n",
    "    with tf.name_scope(\"Opt_and_loss\"):\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_hat))\n",
    "        train_step_mlptest = tf.train.AdamOptimizer(0.01).minimize(cross_entropy)#, var_list=mlp_test_vars)\n",
    "\n",
    "    with tf.name_scope(\"Support_stuff\"):\n",
    "        init = tf.global_variables_initializer()\n",
    "        correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_hat,1))\n",
    "        mlp_acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Someone put together nice code to visualize tensorflow graphs in ipython notebooks. There is more visualization / exploration that can be done using tensorboard, which is the tool this uses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "We visualize our graph: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(mlp_graph.as_graph_def())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a session, initialize our variables, and train the network: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session(graph=mlp_graph)\n",
    "sess.run(init)\n",
    "train_steps = 2500\n",
    "\n",
    "acc = np.zeros(train_steps)\n",
    "\n",
    "# create this op outside of the loop so we don't create it 5000 times\n",
    "for i in range(train_steps):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    acc[i] = mlp_acc.eval(session = sess, feed_dict = {x: batch_xs, y: batch_ys})\n",
    "    sess.run(train_step_mlptest, feed_dict={x: batch_xs, y: batch_ys})\n",
    "\n",
    "test_acc = mlp_acc.eval(session=sess, feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "print(\"Test accuracy: %f\" % test_acc)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use those blocks to construct the VAE. First, some config stuff and a helper: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.distributions import MultivariateNormalDiag, kl\n",
    "\n",
    "encoder_depth = 2\n",
    "decoder_depth = 2\n",
    "encoder_units = 100\n",
    "decoder_units = 100\n",
    "latent_size = 10\n",
    "global_dtype = tf.float32\n",
    "minibatch_size = 500\n",
    "input_size = mnist.train.images.shape[1]\n",
    "train_steps = mnist.train.num_examples // minibatch_size\n",
    "encoder_nonlinearity = tf.nn.tanh\n",
    "decoder_nonlinearity = tf.nn.tanh\n",
    "n_epochs = 1\n",
    "\n",
    "def elbo_op(x, p, q):\n",
    "    prior = MultivariateNormalDiag(tf.zeros([latent_size]), tf.ones(latent_size))\n",
    "    kd = kl(q, prior)\n",
    "    ll = p.log_prob(x)\n",
    "    return tf.reduce_mean(ll + kd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the graph: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vae_graph = tf.Graph()\n",
    "\n",
    "with vae_graph.as_default():\n",
    "    with tf.name_scope(\"Encoder_Q\"):\n",
    "        x = tf.placeholder(shape=[None, input_size], dtype=global_dtype, name='x')\n",
    "        q_mu, q_mu_vars = _mlp(x, n_layers=encoder_depth, units_per_layer=encoder_units, input_size=input_size, out_size=latent_size, nonlinearity=encoder_nonlinearity)\n",
    "        q_logsigma, q_logsigma_vars = _mlp(x, n_layers=encoder_depth, units_per_layer=encoder_units, input_size=input_size, out_size=latent_size, nonlinearity=encoder_nonlinearity)\n",
    "        q = MultivariateNormalDiag(q_mu, 1e-10 + tf.exp(q_logsigma))\n",
    "        epsilon = tf.random_normal([minibatch_size, latent_size])\n",
    "        z = q_mu + tf.sqrt(tf.exp(q_logsigma)) * epsilon\n",
    "\n",
    "    with tf.name_scope(\"Decoder_P\"):\n",
    "        p_mu, p_mu_vars = _mlp(z, n_layers=decoder_depth, units_per_layer=decoder_units, input_size=latent_size,  out_size=input_size, nonlinearity=decoder_nonlinearity)\n",
    "        p_logsigma, p_logsigma_vars = _mlp(z, n_layers=decoder_depth, units_per_layer=decoder_units, input_size=latent_size, out_size=input_size, nonlinearity=decoder_nonlinearity)\n",
    "        p = MultivariateNormalDiag(p_mu, 1e-10 + tf.exp(p_logsigma))\n",
    "\n",
    "    with tf.name_scope(\"Opt_and_loss\"):\n",
    "        elbo = elbo_op(x, p, q)\n",
    "        minimize_op = tf.train.AdamOptimizer(0.005).minimize(-elbo)\n",
    "\n",
    "        \n",
    "    with tf.name_scope(\"Support_stuff\"):\n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "\n",
    "opt_vars = q_mu_vars + q_logsigma_vars + p_mu_vars + p_logsigma_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(vae_graph.as_graph_def())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run and visualize: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session(graph=vae_graph)\n",
    "sess.run(init)\n",
    "\n",
    "elbo_log = np.zeros(n_epochs * train_steps)\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    for j in range(train_steps):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(minibatch_size)\n",
    "        sess.run(minimize_op, feed_dict={x: batch_xs})\n",
    "        elbo_log[i*train_steps + j] = elbo.eval(session=sess, feed_dict={x: batch_xs})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(elbo_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(batch_xs[0].reshape(28,28))\n",
    "plt.figure()\n",
    "generated_mu = p_mu.eval(session=sess, feed_dict={z: np.random.normal(size=(minibatch_size, latent_size))})\n",
    "for i in range(10):\n",
    "    plt.imshow(generated_mu[i].reshape(28,28))\n",
    "    plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
